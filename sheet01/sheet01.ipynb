{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2e647a4",
   "metadata": {},
   "source": [
    "# Exercise Sheet 01: Data preprocessing, visualization, correlation, statistical testing, and modeling\n",
    "\n",
    "**Introduction to Data Mining WS23/24**  \n",
    "**Bielefeld University**  \n",
    "**Alina Deriyeva, Benjamin Paa√üen**  \n",
    "**Exercise Sheet Publication Date: 2023-10-23**  \n",
    "**Exercise Sheet Submission Deadline: 2023-11-03, noon (i.e. 12:00), via git or e-mail to aderiyeva@techfak.uni-bielefeld.de**\n",
    "\n",
    "**NOTE** The use of language models/AI tools is permitted IF you notify us of the use (just indicate it in the respective task) and are still able to understand and present your results. We also appreciate it if you link to a chatlog of the interaction with the language model/AI tool so that we can understand better how students tend to use these tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8967b8",
   "metadata": {},
   "source": [
    "**AUTHORS**\n",
    "\n",
    "Florian Wicher, Christopher Gerz, Torben Lambrecht, Bastian Hunecke"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2591c02c",
   "metadata": {},
   "source": [
    "## Preamble: Data set\n",
    "\n",
    "In this exercise sheet, we investigate first data exploration activities that can be performed on typical, tabular data sets, up to simple statistical testing.\n",
    "\n",
    "The file `sheet01_data.csv` contains fictional data as might be produced in an educational study. Each row represents a student participating in the study. The first column is just the student index, the second column indicates the experimental condition the student was in (`0` for control group, `1` for intervention group). The third column is the student's test result on a pre-test, the fourth column is the student's test result on a post-test.\n",
    "\n",
    "The following line loads this raw data and prints it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0d93f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cc2e58d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.,  27.,  40.],\n",
       "       [  1.,   0.,  31.,  34.],\n",
       "       [  2.,   0.,  30.,  nan],\n",
       "       [  3.,   0.,  20.,  nan],\n",
       "       [  4.,   0.,  41.,  54.],\n",
       "       [  5.,   0.,  39.,  51.],\n",
       "       [  6.,   0.,  20.,  33.],\n",
       "       [  7.,   0.,  27.,  36.],\n",
       "       [  8.,   0.,  79.,  83.],\n",
       "       [  9.,   0.,  33.,  46.],\n",
       "       [ 10.,   0.,  22.,  29.],\n",
       "       [ 11.,   0.,  22.,  41.],\n",
       "       [ 12.,   0.,  29.,  nan],\n",
       "       [ 13.,   0.,  25.,  nan],\n",
       "       [ 14.,   0.,  41.,  47.],\n",
       "       [ 15.,   0.,  23.,  36.],\n",
       "       [ 16.,   0.,  17.,  30.],\n",
       "       [ 17.,   0.,  93., 100.],\n",
       "       [ 18.,   0.,  34.,  40.],\n",
       "       [ 19.,   0.,  23.,  27.],\n",
       "       [ 20.,   0.,  48.,  53.],\n",
       "       [ 21.,   0.,  19.,  26.],\n",
       "       [ 22.,   0.,  28.,  33.],\n",
       "       [ 23.,   0.,  38.,  46.],\n",
       "       [ 24.,   0.,  22.,  34.],\n",
       "       [ 25.,   0.,  34.,  45.],\n",
       "       [ 26.,   0.,  36.,  49.],\n",
       "       [ 27.,   0.,  33.,  47.],\n",
       "       [ 28.,   0.,  40.,  41.],\n",
       "       [ 29.,   0.,  39.,  52.],\n",
       "       [ 30.,   1.,  24.,  50.],\n",
       "       [ 31.,   1.,  38.,  62.],\n",
       "       [ 32.,   1.,  34.,  51.],\n",
       "       [ 33.,   1.,  37.,  nan],\n",
       "       [ 34.,   1.,  31.,  44.],\n",
       "       [ 35.,   1.,  37.,  57.],\n",
       "       [ 36.,   1.,  25.,  50.],\n",
       "       [ 37.,   1.,  23.,  37.],\n",
       "       [ 38.,   1.,  12.,  38.],\n",
       "       [ 39.,   1.,  31.,  56.],\n",
       "       [ 40.,   1.,  36.,  46.],\n",
       "       [ 41.,   1.,  34.,  48.],\n",
       "       [ 42.,   1.,  85., 100.],\n",
       "       [ 43.,   1.,  41.,  59.],\n",
       "       [ 44.,   1.,  14.,  37.],\n",
       "       [ 45.,   1.,   7.,  29.],\n",
       "       [ 46.,   1.,  30.,  54.],\n",
       "       [ 47.,   1.,  42.,  62.],\n",
       "       [ 48.,   1.,  25.,  44.],\n",
       "       [ 49.,   1.,  29.,  53.],\n",
       "       [ 50.,   1.,  17.,  40.],\n",
       "       [ 51.,   1.,  40.,  63.],\n",
       "       [ 52.,   1.,  29.,  46.],\n",
       "       [ 53.,   1.,  33.,  nan],\n",
       "       [ 54.,   1.,  24.,  50.],\n",
       "       [ 55.,   1.,  38.,  53.],\n",
       "       [ 56.,   1.,  26.,  48.],\n",
       "       [ 57.,   1.,  41.,  nan],\n",
       "       [ 58.,   1.,  37.,  65.],\n",
       "       [ 59.,   1.,  41.,  61.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.loadtxt('sheet01_data.csv', skiprows = 1, delimiter = '\\t')\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e93d467",
   "metadata": {},
   "source": [
    "### Task 01.01\n",
    "\n",
    "Write python code to automatically identify outliers, which are defined as any students with a pre-test score higher than 3 standard deviations above the mean. Write python code that removes these outliers from the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b3b0e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67a93666",
   "metadata": {},
   "source": [
    "Print how many points are left in the control group and the intervention group after outlier removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a11dc77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4d3cbdb",
   "metadata": {},
   "source": [
    "### Task 01.02\n",
    "\n",
    "Write python code to compute the mean pre- and post-test score, as well as the respective standard deviation, of the control group and the intervention group. Be aware of nan values. Print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cc8bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f98eecb2",
   "metadata": {},
   "source": [
    "### Task 01.03\n",
    "\n",
    "Write python code to impute the missing values (that means: nan values) in the post test scores by the mean of the data. HOWEVER, the imputation should be done separately for the control and the intervention group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc00cfc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6dc11501",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14b18c1",
   "metadata": {},
   "source": [
    "### Task 01.04\n",
    "\n",
    "Display two scatter plots (via `matplotlib.pyplot.scatter`), one for the control group and one for the intervention group, with pre-test score on the x-axis and post-test score on the y-axis. Label the axis and give the plots titles. Interpret these plot: Do you believe that pre- and post-test score correlate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d743b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fde7831",
   "metadata": {},
   "source": [
    "Yes, I believe that pre- and post-test scores correlate because post-test scores tend to be higher when pre-test scores are higher and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb85469c",
   "metadata": {},
   "source": [
    "### Task 01.05\n",
    "\n",
    "Display a bar plot with four bars (with error bars): pre-test mean (and standard error) for control and intervention group; and post-test mean (with standard error) for control and intervention group. Label the axes. Interpret this plot: Where are significant differences, do you think? What is your explanation for these differences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b449f058",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02752dcc",
   "metadata": {},
   "source": [
    "I think control pre and intervention pre do not differ.\n",
    "control post is higher than both control pre and intervention pre because the students learned something between pre- and post-test.\n",
    "intervention post is even higher than control post because students learned more in the intervention than in the control condition.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c8b00a",
   "metadata": {},
   "source": [
    "### Task 01.06\n",
    "\n",
    "Display a bar plot with two bars (with error bars): the difference between post- and pre-test score for the control group (with standard error) and for the intervention group (with standard error). Label the axes. Interpret this plot: Do you think the difference of post- and pre-test scores significantly differs between control and intervention group?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95af15e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3886ad85",
   "metadata": {},
   "source": [
    "Yes, I think the score difference is significantly higher in the intervention group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296e5b5e",
   "metadata": {},
   "source": [
    "### Task 01.07 (Bonus task)\n",
    "\n",
    "Display a box plot with four boxes: pre-test results in the control and intervention group, and post-test results in the control and intervention group. Label the axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6ec57c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "874ce4ea",
   "metadata": {},
   "source": [
    "### Task 01.08 (Bonus Task)\n",
    "\n",
    "Display a box plot with two boxes: difference between post-test and pre-test scores for the control group and the intervention group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084699cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e4a6bbf",
   "metadata": {},
   "source": [
    "## Statistical Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2679fdb5",
   "metadata": {},
   "source": [
    "### Task 01.09\n",
    "\n",
    "Write a python function to compute the Pearson correlation between two arrays of the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "416d471f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.4166741488366063"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_a = np.random.rand(5)\n",
    "array_b = np.random.rand(5)\n",
    "\n",
    "matrix_a = np.random.rand(5,2)\n",
    "matrix_b = np.random.rand(5,2)\n",
    "\n",
    "\n",
    "#wikipedia def\n",
    "def pearson_corr_a(a,b):\n",
    "    return np.divide((np.cov(a,b)), (np.multiply(np.var(a), np.var(b))))\n",
    "\n",
    "#vl def\n",
    "def linear_correlation_coeff(x,y):\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    m = x.size\n",
    "    \n",
    "    r = 0\n",
    "    \n",
    "    mu_x = np.mean(x)\n",
    "    mu_y = np.mean(y)\n",
    "    \n",
    "    sigma_x = np.std(x)\n",
    "    sigma_y = np.std(y)\n",
    "    \n",
    "    for i in range(m):\n",
    "        r += ((x[i] - mu_x) / sigma_x) * ((y[i] - mu_y) / sigma_y)\n",
    "        \n",
    "    return r / m\n",
    "\n",
    "linear_correlation_coeff(array_a, array_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc5b7e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('sheet01_data.csv', delimiter='\\t')\n",
    "\n",
    "#hier hab ich zum weitermachen die nan's ersetzt\n",
    "data = data.fillna(data.post_test.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b451ceff",
   "metadata": {},
   "source": [
    "### Task 01.10\n",
    "\n",
    "Use your function to compute the Pearson correlation between pre-test and post-test scores for both control and intervention condition and print the scores.\n",
    "\n",
    "Interpret the strength of the correlation using the following rule of thumb from [Mukaka (2012)](https://www.ajol.info/index.php/mmj/article/download/81576/71739):\n",
    "\n",
    "* A correlation of $0.3 < |r| \\leq 0.5$ is considered small.\n",
    "* A correlation of $0.5 < |r| \\leq 0.7$ is considered moderate.\n",
    "* A correlation of $0.7 < |r| \\leq 0.9$ is considered high.\n",
    "* A correlation of $0.9 < |r| \\leq 1.0$ is considered very high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95846faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R:  0.8794675662078608\n",
      "High correlation\n"
     ]
    }
   ],
   "source": [
    "pre_test = data.pre_test.values\n",
    "post_test = data.post_test.values\n",
    "\n",
    "r =  np.abs(linear_correlation_coeff(pre_test, post_test))\n",
    "print(\"R: \", r)\n",
    "\n",
    "if r > 0.3 and r <= 0.5:\n",
    "    print(\"Small correlation\")\n",
    "elif r > 0.5 and r <= 0.7:\n",
    "    print(\"Moderate correlation\")\n",
    "elif r > 0.7 and r <= 0.9:\n",
    "    print(\"High correlation\")\n",
    "elif r > 0.9 and r <= 1.0:\n",
    "    print(\"Very High correlation\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22259ca3",
   "metadata": {},
   "source": [
    "Validate your scores by computing the Pearson correlation again with `scipy.stats.pearsonr`. Print both `r` and the `p` value returned by the function for both the control and the intervention condition. Are the correlations statistically significant at $0.01$ level?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dce1a401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r =  0.8794675662078609\n",
      "p =  2.3499726099678504e-20\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "stats = stats.pearsonr(pre_test, post_test)\n",
    "\n",
    "print(\"r = \", stats[0])\n",
    "print(\"p = \", stats[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a73b160",
   "metadata": {},
   "source": [
    "### Task 01.11\n",
    "\n",
    "Write a python function that performs a Welch $t$-test. In particular, your function should:\n",
    "1. compute the number of data points, the mean, and the standard deviation of both samples (use the `ddof = 1` parameter for the `np.std` function).\n",
    "2. compute the pooled standard deviation as\n",
    "\\begin{equation}\n",
    "\\sigma_\\text{pooled} = \\sqrt{\\frac{\\sigma_x^2}{n_x} + \\frac{\\sigma_y^2}{n_y}}\n",
    "\\end{equation}\n",
    "  where $n_x$ and $n_y$ are the number of data points in the two samples and $\\sigma_x$ and $\\sigma_y$ are the respective standard deviations.\n",
    "3. compute the $t$-statistic as \n",
    "\\begin{equation}\n",
    "t = -\\Big|\\frac{\\mu_x - \\mu_y}{\\sigma_\\text{pooled}}\\Big|\n",
    "\\end{equation}\n",
    "  where $\\mu_x$ and $\\mu_y$ are the means of the two samples.\n",
    "4. compute the number of degree of freedom via the Welch-Satterthwaite equation, meaning:\n",
    "\\begin{equation}\n",
    "\\text{df} = \\frac{\\sigma_\\text{pooled}^4}{\\frac{\\sigma_x^4}{n_x^2\\cdot(n_x - 1)} + \\frac{\\sigma_y^4}{n_y^2\\cdot(n_y - 1)}}\n",
    "\\end{equation}\n",
    "5. compute $p$ as twice the probability of any $t$ value equal or smaller to your value using the `cdf` function of `scipy.stats.t` with the number of freedoms as computed in the previous step.\n",
    "\n",
    "Your function should return both $t$ and $p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93892529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import t\n",
    "#1\n",
    "def compute_pooled_std(x,y):\n",
    "    \n",
    "    #compute size of arrays and std\n",
    "    size_x = x.size\n",
    "    size_y = y.size\n",
    "    \n",
    "    std_x = np.std(x, ddof = 1)\n",
    "    std_y = np.std(y, ddof = 1)\n",
    "    \n",
    "    op_a = np.square(std_x) / size_x\n",
    "    op_b = np.square(std_y) / size_y\n",
    "    \n",
    "    return np.sqrt((op_a+op_b))\n",
    "\n",
    "def compute_t(x,y):\n",
    "    \n",
    "    mean_x = np.mean(x)\n",
    "    mean_y = np.mean(y)\n",
    "    \n",
    "    pooled_std = compute_pooled_std(x,y)\n",
    "    \n",
    "    t = -np.abs( ( (mean_x - mean_y) / pooled_std))\n",
    "    \n",
    "    return t\n",
    "\n",
    "def compute_df(x,y):\n",
    "    \n",
    "    pooled_std = compute_pooled_std(x,y)\n",
    "    \n",
    "    std_x_4 = np.std(x, ddof = 1)**4\n",
    "    std_y_4 = np.std(y, ddof = 1)**4\n",
    "    \n",
    "    size_x = x.size\n",
    "    size_y = y.size\n",
    "    \n",
    "    op_a = std_x_4 / (size_x**2 * (size_x-1))\n",
    "    op_b = std_y_4 / (size_y**2 * (size_y-1))\n",
    "    \n",
    "    df =  (pooled_std ** 4) / (op_a + op_b)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_p_t(x,y):\n",
    "    \n",
    "    t_v = compute_t(x,y)\n",
    "    print(\"t = \", t_v)\n",
    "    \n",
    "    df = compute_df(x,y)\n",
    "    print(\"df = \", df)\n",
    "    \n",
    "    p = t.cdf(t_v, df)\n",
    "    print(\"p = \", p*2)\n",
    "    \n",
    "    return (t, p*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c0817fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t =  -0.35519229868081476\n",
      "df =  7.805461452668546\n",
      "p =  0.7318519023294376\n",
      "##########\n",
      "Correct t =  -0.35519229868081476\n",
      "Correct p =  0.7318519023294379\n"
     ]
    }
   ],
   "source": [
    "a = np.random.rand(5)\n",
    "b = np.random.rand(5)\n",
    "\n",
    "t,p = compute_p_t(a,b)\n",
    "\n",
    "#validation\n",
    "\n",
    "val_result = scipy.stats.ttest_ind(a,b, equal_var=False)\n",
    "\n",
    "print('#'*10)\n",
    "print(\"Correct t = \", val_result[0])\n",
    "print(\"Correct p = \", val_result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da322421",
   "metadata": {},
   "source": [
    "### Task 01.12\n",
    "\n",
    "Write python code to perform the following Welch tests (using your function):\n",
    "1. the pre-test scores in the control versus the intervention condition\n",
    "2. the post-test scores in the control versus the intervention condition\n",
    "3. the differences between post- and pre-test scores in the control versus the intervention condition\n",
    "\n",
    "For each of the tests, print the $t$ and $p$.\n",
    "\n",
    "Which of the test results are significant at a $0.01$-level?\n",
    "\n",
    "**Hint:** You can validate that your function returns the correct $t$ and $p$ values by comparing to the output of the function `scipy.stats.ttest_ind` with `equal_var = False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b752429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=-0.35519229868081476, pvalue=0.7318519023294379)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = scipy.stats.ttest_ind(a,b, equal_var=False)\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cfc77a",
   "metadata": {},
   "source": [
    "### Task 01.13\n",
    "\n",
    "Compute the effect sizes for the difference in post-test minus pre-test score between control and intervention condition using the formula\n",
    "\n",
    "\\begin{equation}\n",
    "d = \\frac{|\\mu_x - \\mu_y|}{\\sqrt{\\frac{1}{2}(\\sigma_x^2 + \\sigma_y^2)}}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mu_x$ and $\\mu_y$ are the mean score difference in intervention and control condition and $\\sigma_x$ and $\\sigma_y$ are the respective standard deviations.\n",
    "\n",
    "Print the effect size and interpret the size of the effect using the following rule of thumb (by [Sawilowsky (2009)](https://digitalcommons.wayne.edu/cgi/viewcontent.cgi?article=1536&context=jmasm))\n",
    "* An effect of size $0.01 < d \\leq 0.2$ is considered very small.\n",
    "* An effect of size $0.2 < d \\leq 0.5$ is considered small.\n",
    "* An effect of size $0.5 < d \\leq 0.8$ is considered moderate.\n",
    "* An effect of size $0.8 < d \\leq 1.2$ is considered large.\n",
    "* An effect of size $1.2 < d \\leq 2.0$ is considered very large.\n",
    "* An effect of size $d > 2.0$ is considered huge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132d8c01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ee25242",
   "metadata": {},
   "source": [
    "### Task 01.14 (Bonus Task)\n",
    "\n",
    "Write python code that uses the wilcoxon sign rank test (via `scipy.stats.wilcoxon`) to check whether the post-test scores in the control condition are significantly different from the pre-test scores. Repeat the same analysis for the intervention condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa61be4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86c1e04e",
   "metadata": {},
   "source": [
    "## Probabilistic Modeling\n",
    "\n",
    "Let's assume that a student does a test in which they can achieve a certain number of points in the real numbers. Further, assume that the number of points achieved is generated from a Gaussian distribution with standard deviation $\\sigma$. However, the mean of the distribution depends on whether the student has understood the topic or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfcecc0",
   "metadata": {},
   "source": [
    "### Task 01.15\n",
    "\n",
    "Formalize this scenario. In particular:\n",
    "1. Model a random variable $Z$ that indicates whether the student understood the topic or not. What is the domain? What is the probability mass function $p_Z$? What are the parameters of this function?\n",
    "2. Model a random variable $X$ for the number of points the student achieves on the test. What is the conditional probability density function $p_{X|Z}$? What are the parameters of this function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7757c619",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0540b4aa",
   "metadata": {},
   "source": [
    "### Task 01.16\n",
    "\n",
    "Using Bayes' theorem and the law of total probability, compute the conditional probability mass $p_{Z|X}(1|x)$ for the student having understood the topic given their achieved number of points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42722ba8",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6195d94c",
   "metadata": {},
   "source": [
    "### Task 01.17\n",
    "\n",
    "Assume that the numbers $x_1, \\ldots, x_m \\in \\mathbb{R}$ are generated (independently) from a Gaussian with mean $\\mu$ and standard deviation $\\sigma$.\n",
    "\n",
    "1. What is the negative log likelihood of the data under this assumption?\n",
    "2. Prove that $\\mu$ and $\\sigma^2$ which minimize the negative log likelihood are exactly $\\mu = \\frac{1}{m} \\sum_{i=1}^m x_i$ and $\\sigma^2 = \\frac{1}{m} \\sum_{i=1}^m (x_i - \\mu)^2$. You may assume that the negative log likelihood is convex, in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e25d24",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
